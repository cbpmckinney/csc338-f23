{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "def SoftMax(values):\n",
    "    answers = [0]*len(values)\n",
    "    denom = 0\n",
    "    for i in range(len(values)):\n",
    "        denom += math.exp(values[i])\n",
    "    for i in range(len(values)):\n",
    "        answers[i] = math.exp(values[i])/denom\n",
    "    return answers\n",
    "\n",
    "def Sigmoid(values): #This function does sigmoid on all the values, not just one at a time\n",
    "    answers = [0]*len(values)\n",
    "    for i in range(len(values)):\n",
    "        answers[i] = 1/(1+math.exp(-values[i]))\n",
    "    return answers\n",
    "\n",
    "\n",
    "\n",
    "def OneHot(label):\n",
    "    ans = [0]*10\n",
    "    ans[label] = 1\n",
    "    return ans\n",
    "\n",
    "\n",
    "class neuralnet:\n",
    "\n",
    "    def __init__(self, numinputs, numoutputs, errorfunc=\"MSE\"):\n",
    "\n",
    "        self.numinputs = numinputs\n",
    "        self.numoutputs = numoutputs\n",
    "        self.errorfunc = errorfunc\n",
    "        \n",
    "        self.biases = np.random.default_rng().random(self.numoutputs)\n",
    "        self.weights = np.random.default_rng().random((self.numoutputs, self.numinputs))\n",
    "\n",
    "        if self.errorfunc == \"MSE\":\n",
    "            self.biases /= 1000\n",
    "            self.weights /= 1000\n",
    "\n",
    "\n",
    "    def evaluate(self, inputs):\n",
    "            try:\n",
    "                output = self.weights @ inputs + self.biases\n",
    "                return output\n",
    "            except:\n",
    "                print(\"Dimension mismatch!\")\n",
    "\n",
    "    def computeerror(self, outputs, trueoutputs):\n",
    "        if self.errorfunc == \"MSE\":\n",
    "\n",
    "            sum = 0\n",
    "            for i in range(len(outputs)):\n",
    "                sum += (outputs[i] - trueoutputs[i])**2\n",
    "            sum = sum * (1/(2*self.numoutputs))\n",
    "            return sum \n",
    "\n",
    "        if self.errorfunc == \"CrossEntropy\":\n",
    "            sum = 0\n",
    "            for i in range(0, len(outputs)):\n",
    "                sum += -trueoutputs[i]*math.log2(outputs[i])\n",
    "            return sum\n",
    "\n",
    "\n",
    "    def computegradient(self, inputs, labels):\n",
    "\n",
    "        wparts = np.zeros((self.numoutputs, self.numinputs))\n",
    "        bparts = np.zeros(self.numoutputs)\n",
    "\n",
    "\n",
    "        ys = self.weights @ inputs + self.biases # We need this regardless of the error function\n",
    "        M3 = np.zeros((self.numoutputs, self.numoutputs, self.numinputs)) # Ditto\n",
    "        for i in range(self.numoutputs):\n",
    "            for j in range(self.numinputs):\n",
    "                M3[i,i,j] = inputs[j]\n",
    "\n",
    "        if self.errorfunc == \"MSE\":\n",
    "            \n",
    "            # You need to put stuff here\n",
    "\n",
    "            zs = Sigmoid(ys)\n",
    "\n",
    "            M2 = np.zeros((self.numoutputs, self.numoutputs))\n",
    "            for i in range(self.numoutputs):\n",
    "                for j in range(self.numoutputs):\n",
    "                    if (i == j):\n",
    "                        M2[i,j] = math.exp(-ys[1])/(1+math.exp(-ys[1]))**2\n",
    "\n",
    "            M1 = np.zeros((self.numoutputs))\n",
    "            for i in range(self.numoutputs):\n",
    "                M1[i] = 1 / self.numoutputs * (zs[i] - labels[i])\n",
    "        \n",
    "\n",
    "            wparts = M1 @ (M2 @ M3)\n",
    "            bparts = M1 @ M2\n",
    "\n",
    "\n",
    "            return (wparts, bparts)\n",
    "\n",
    "\n",
    "        if self.errorfunc == \"CrossEntropy\":\n",
    "\n",
    "            softdenom = 0\n",
    "\n",
    "            for i in range(len(ys)):\n",
    "                softdenom += math.exp(ys[i])\n",
    "\n",
    "            zs = SoftMax(ys)\n",
    "            \n",
    "\n",
    "            M2 = np.zeros((self.numoutputs, self.numoutputs))\n",
    "            for i in range(self.numoutputs):\n",
    "                for j in range(self.numoutputs):\n",
    "                    if (i == j):\n",
    "                        M2[i,j] = ((softdenom)*math.exp(ys[j]) - math.exp(2*ys[j])) / (softdenom**2)\n",
    "                    else:\n",
    "                        M2[i,j] = -(math.exp(ys[i]+ys[j])) / (softdenom ** 2)\n",
    "\n",
    "            M1 = np.zeros((self.numoutputs))\n",
    "            for i in range(self.numoutputs):\n",
    "                M1[i] = -labels[i] / (zs[i] * math.log(2))\n",
    "        \n",
    "\n",
    "            wparts = M1 @ (M2 @ M3)\n",
    "            bparts = M1 @ M2\n",
    "            \n",
    "            return (wparts, bparts)\n",
    "\n",
    "       \n",
    "\n",
    "        \n",
    "    \n",
    "    def updateweights(self, wparts, bparts, eta):\n",
    "\n",
    "        # This doesn't need to be changed, since all it takes in are\n",
    "        # the partial derivatives computed elsewhere\n",
    "\n",
    "        self.weights -= eta*wparts\n",
    "        self.biases -= eta*bparts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1 0.2]\n",
      " [0.3 0.4]]\n",
      "[0.7 0.8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.87, 1.19])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiate network and set weights/biases\n",
    "NNAssignment4 = neuralnet(2,2,\"CrossEntropy\")\n",
    "NNAssignment4.weights = np.array([[0.1, 0.2], [0.3, 0.4]])\n",
    "NNAssignment4.biases = np.array([0.7, 0.8])\n",
    "\n",
    "\n",
    "print(NNAssignment4.weights)\n",
    "print(NNAssignment4.biases)\n",
    "\n",
    "NNAssignment4.weights @ np.array([0.5, 0.6]) + NNAssignment4.biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.41789411 -0.50147294]\n",
      " [ 0.41789411  0.50147294]]\n",
      "[-0.83578823  0.83578823]\n"
     ]
    }
   ],
   "source": [
    "(wparts, bparts) = NNAssignment4.computegradient([0.5,0.6],[1,0])\n",
    "\n",
    "print(wparts)\n",
    "print(bparts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.512946920393323, 0.5132761714827284, 0.5142314800036145, 0.5136748669706976, 0.5129043715096221, 0.513605022825544, 0.5127035476962976, 0.5153045697172077, 0.5122467562710654, 0.5133212103467188]\n",
      "0.012873531653128179\n"
     ]
    }
   ],
   "source": [
    "# Assignment 5 testing\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Define two versions of our neural net\n",
    "\n",
    "MyNeuralNet1 = neuralnet(784,10,\"CrossEntropy\")\n",
    "MyNeuralNet2 = neuralnet(784,10,\"MSE\")\n",
    "\n",
    "\n",
    "image = (train_images[0].astype(float)/255).flatten()\n",
    "label = OneHot(train_labels[0])\n",
    "(wparts, bparts) = MyNeuralNet2.computegradient(image, label)\n",
    "\n",
    "output = MyNeuralNet2.evaluate(image)\n",
    "\n",
    "print(Sigmoid(output))\n",
    "\n",
    "print(np.matrix(wparts).max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell once, to train each network over 10 epochs.  Then run the next cell to compute the errors.\n",
    "# THEN: Re-run this cell and the next cell to see how the error decreases for each NN.\n",
    "\n",
    "# Train Neural Net 1, 1000 images, over 10 epochs\n",
    "\n",
    "for j in range(10):\n",
    "    for i in range(len(train_images)):\n",
    "        image = (train_images[i].astype(float)/255).flatten()\n",
    "        label = OneHot(train_labels[i])\n",
    "        (wparts, bparts) = MyNeuralNet1.computegradient(image, label)\n",
    "        MyNeuralNet1.updateweights(wparts, bparts, 0.01)\n",
    "\n",
    "\n",
    "# Train Neural Net 2, 1000 images, over 10 epochs\n",
    "\n",
    "for j in range(10):\n",
    "    for i in range(len(train_images)):\n",
    "        image = (train_images[i].astype(float)/255).flatten()\n",
    "        label = OneHot(train_labels[i])\n",
    "        (wparts, bparts) = MyNeuralNet2.computegradient(image, label)\n",
    "        MyNeuralNet2.updateweights(wparts, bparts, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumulativeerror1 = 0\n",
    "cumulativeerror2 = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    test1 = (test_images[i].astype(float)/255).flatten()\n",
    "    label1 = OneHot(test_labels[i])\n",
    "    out1 = SoftMax(MyNeuralNet1.evaluate(test1))\n",
    "    cumulativeerror1 += MyNeuralNet1.computeerror(out1, label1)\n",
    "\n",
    "for i in range(1000):\n",
    "    test2 = (test_images[i].astype(float)/255).flatten()\n",
    "    label2 = OneHot(test_labels[i])\n",
    "    out2 = Sigmoid(MyNeuralNet2.evaluate(test2))\n",
    "    cumulativeerror2 += MyNeuralNet2.computeerror(out2, label2)\n",
    "\n",
    "#print(cumulativeerror1)\n",
    "#print(cumulativeerror2)\n",
    "\n",
    "\n",
    "#print(test_labels[0])\n",
    "test2 = Sigmoid(MyNeuralNet2.evaluate((test_images[0].astype(float)/255).flatten()))\n",
    "test3 = SoftMax(MyNeuralNet1.evaluate((test_images[0].astype(float)/255).flatten()))\n",
    "#print(test2)\n",
    "#print(test3)\n",
    "\n",
    "MyNeuralNet2.computeerror(test2, OneHot(test_labels[0]))\n",
    "\n",
    "\n",
    "test2.index(np.matrix(test2).max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass Rate 1:  0.8474\n",
      "Pass Rate 2:  0.61\n"
     ]
    }
   ],
   "source": [
    "# Compute pass rate\n",
    "\n",
    "passrate1 = 0\n",
    "passrate2 = 0\n",
    "\n",
    "for i in range(len(test_images)):\n",
    "\n",
    "    result1 = SoftMax(MyNeuralNet1.evaluate((test_images[i].astype(float)/255).flatten()))\n",
    "    result2 = Sigmoid(MyNeuralNet2.evaluate((test_images[i].astype(float)/255).flatten()))\n",
    "    label = OneHot(test_labels[i])\n",
    "\n",
    "    if result1.index(np.matrix(result1).max()) == test_labels[i]:\n",
    "        passrate1 += 1\n",
    "    if result2.index(np.matrix(result2).max()) == test_labels[i]:\n",
    "        passrate2 += 1\n",
    "    \n",
    "\n",
    "print(\"Pass Rate 1: \", passrate1/len(test_images))\n",
    "print(\"Pass Rate 2: \", passrate2/len(test_images))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
